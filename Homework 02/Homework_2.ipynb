{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d877b276",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/r-doz/PML2025/blob/main/./Homeworks/Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Machine Learning -- Spring 2025, UniTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Marginal Likelihood Optimisation\n",
    "\n",
    "Given the Bayesian Linear Regression model implemented in the Notebook 6, considering the same dataset and variables (Apparent Temperature vs Humidity), optimize alpha and beta by maximizing the Marginal Likelihood.\n",
    "\n",
    "Note: You can show here only the piece of code that you used and write the optimal alpha and beta that you obtained (so you can run it in directly on Notebook 6)\n",
    "\n",
    "Hint: import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Rejection Sampling\n",
    "\n",
    "Consider the unnormalized probability density function:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\exp\\left(-\\frac{x^4}{4} - \\frac{x^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "This density is defined for $x \\in \\mathbb{R}$, but it is not normalized. Let $Z$ be its normalization constant:\n",
    "\n",
    "$$\n",
    "Z = \\int_{-\\infty}^{\\infty} \\tilde{p}(x)\\, dx\n",
    "$$\n",
    "\n",
    "You are given access to a proposal distribution $q(x) = \\mathcal{N}(0, 1)$ from which you can sample and evaluate its density.\n",
    "\n",
    "-  Implement and then use rejection sampling with $q(x)$ to generate samples from the normalized target distribution $p(x) = \\frac{1}{Z} \\tilde{p}(x)$.\n",
    "    \n",
    "- Estimate the normalization constant $Z$ (hint: see your course notes!)\n",
    "\n",
    "- Compare your result with a numerical approximation of $Z$ using integration methods (e.g., scipy.integrate.quad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: MCMC Convergence Diagnostics\n",
    "\n",
    "You have implemented a Metropolis-Hastings algorithm and used it to draw samples from a univariate target distribution. In this exercise, you will compute convergence diagnostics from scratch to assess whether your chains have mixed well.\n",
    "\n",
    "Consider 4 parallel chains, each of length $N$ (after burn-in), starting from different initial values (use\n",
    "initial_values = [-10, -2, 2, 10]). The unnormalized probability density function $\\tilde{p}(x)$ is defined as a mixture of two Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\mathcal{N}(x; -5, 1) + \\mathcal{N}(x; 5, 1)\n",
    "$$\n",
    "\n",
    "Perform the following steps two times, considering a proposal standard deviation of 0.1 and 2.0:\n",
    "\n",
    "- Compute the within variance $W$ and the between variance $B$\n",
    "- Compute the statistics $\\hat{R}$\n",
    "- For a single chain of samples $x_1, x_2, \\dots, x_N$, compute the lag-$k$ autocorrelation $\\rho_k$ and plot $\\rho_k$ for $k = 1, 2, \\dots, 20$\n",
    "- Estimate the effective number of samples $n_{eff}$\n",
    "\n",
    "Repeat the analysis with the distribution:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\mathcal{N}(x; -2, 1) + \\mathcal{N}(x; 2, 1)\n",
    "$$\n",
    "\n",
    "Discuss the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: ADSAI football matches\n",
    "Over the years, the PhD students of ADSAI have kept track of the results of their evening five-a-side football matches. Since itâ€™s difficult to always organize teams with the same players, the teams were formed on a rotating basis, each time with different players.  \n",
    "The names of our champions have been anonymized using numbers from 1 to 62.  \n",
    "In the dataset located at `data/ADSAI_football.csv` in the GitHub repository, you will find the following columns:\n",
    "- **Team A**: IDs of the players who played in Team A in that match;  \n",
    "- **Team B**: same as above, for Team B;  \n",
    "- **Goal A**: total goals scored by Team A in that match;  \n",
    "- **Goal B**: same as above, for Team B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your goal is to model **the skill of each player** based on information about the team they belonged to and the overall result achieved by that team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model to implement consists in the following structure: \n",
    "- $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62}) \\in \\mathbb{R} $: players' skills.\n",
    "- $M=54$: number of matches.\n",
    "- For each match $ i = 1, \\dots, M=54$:\n",
    "  - $ A_i \\subset \\{1, \\dots, N=62\\} $: set of players IDs of team A in match $ i $.\n",
    "  - $ B_i \\subset \\{1, \\dots, N=62\\} $: set of players IDs of team B in match $ i $.\n",
    "  - $ y_i \\in \\mathbb{Z} $: observed outcome, i.e. goal difference between the two teams, defined as $ (\\text{goal}_A - \\text{goal}_B) $ in match $ i $.\n",
    "\n",
    "*(in this exercise, you are asked to follow the proposal of Karlis and Ntzoufras approach, that focuses on the goal difference in place of the individual goal counts of each team!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model is defined as follows:\n",
    "\n",
    "- The \"strength\" of Team A, defined as the sum of the individual players' skills $\\theta_j$ composing the team in match $i = 1, \\dots, M=54$:  \n",
    "  $$\n",
    "  s_A^{(i)} = \\sum_{j \\in A_i} \\theta_j\n",
    "  $$\n",
    "\n",
    "- The \"strength\" of Team B, defined similarly as the sum of the individual players' skills $\\theta_j$ composing the team in match $i = 1, \\dots, M=54$:  \n",
    "  $$\n",
    "  s_B^{(i)} = \\sum_{j \\in B_i} \\theta_j\n",
    "  $$\n",
    "\n",
    "Specifically, the observed goal difference in match $i$ is modeled using a Skellam distribution:  \n",
    "$$\n",
    "y_i \\sim \\text{Skellam}(\\lambda_A^{(i)}, \\lambda_B^{(i)}), \\quad \\text{where} \\quad \\lambda_A^{(i)} = \\exp(s_A^{(i)}), \\quad \\lambda_B^{(i)} = \\exp(s_B^{(i)})\n",
    "$$\n",
    "\n",
    "The **Skellam distribution** models the difference between two independent random variables:  \n",
    "$$\n",
    "\\text{Skellam}(\\lambda_A, \\lambda_B) = \\text{Poisson}(\\lambda_A) - \\text{Poisson}(\\lambda_B)\n",
    "$$\n",
    "It is formally defined as:  \n",
    "$$\n",
    "\\text{Skellam}(k; \\lambda_A, \\lambda_B) = e^{-(\\lambda_A + \\lambda_B)} \\left( \\frac{\\lambda_A}{\\lambda_B} \\right)^{k/2} I_{|k|}(2 \\sqrt{\\lambda_A \\lambda_B})\n",
    "$$\n",
    "\n",
    "for each $ k \\in \\mathbb{Z} $, and $ I_k $ is the modified Bessel function of the first kind of order $ k $.\n",
    "\n",
    "$$\n",
    "I_k(z) = \\sum_{m=0}^\\infty \\frac{1}{m! \\, \\Gamma(m + k + 1)} \\left( \\frac{z}{2} \\right)^{2m + k}\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import only relevant quantities as follows:\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "def preprocessing_dataset(dataset_path='ADSAI_football.csv'):\n",
    "    football = pd.read_csv(dataset_path)\n",
    "    football['Team A'] = football['Team A'].apply(ast.literal_eval)\n",
    "    football['Team B'] = football['Team B'].apply(ast.literal_eval)\n",
    "\n",
    "    max_player_id = max(\n",
    "    max(p for team in football['Team A'] for p in team),\n",
    "    max(p for team in football['Team B'] for p in team)\n",
    "    )\n",
    "\n",
    "    goal_diff = torch.tensor((football['Goal A'] - football['Goal B']).values, dtype=torch.int)\n",
    "\n",
    "    teams_A = [torch.tensor(team) for team in football['Team A']]\n",
    "    teams_B = [torch.tensor(team) for team in football['Team B']]\n",
    "\n",
    "    return teams_A, teams_B, goal_diff, max_player_id\n",
    "\n",
    "\n",
    "teams_A, teams_B, goal_diff, max_player_id = preprocessing_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You are asked to: \n",
    "1. Find the MAP estimate for $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ choosing as `log_prior` function a (log-)Standard Normal distribution and as `log_likelihood` function the (log-)Skellam. Perform optimization using Gradient Descent. Use the MAP estimate to implement a Laplace approximation of the posterior, as done during the lab (reuse the function `compute_hessian(f, w)` of Notebook 6).\n",
    "\n",
    "**Important Hint:** The Skellam log-likelihood involves the modified Bessel function $I_k(z)$, which is non-differentiable in PyTorch if evaluated via scipy. To preserve differentiability, replace $\\log I_k(z)$ with a smooth approximation, such as an asymptotic expansion, to allow gradient-based optimization.\n",
    "\n",
    "2. **(Useful for the next point, not strictly necessary for the previous one):** Implement your `Skellam` distribution, inheriting from `torch.distributions.Distribution`;\n",
    "3. Write the Pyro model corresponding to the problem depicted above assuming (again) the `theta` values being distributed initially as a Standard Normal;\n",
    "4. Perform inference on $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ values running a MCMC simulation using the `NUTS` kernel;\n",
    "5. Compare the `theta` values obtained by these two options using the `performances_evaluation` function given in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Find the MAP estimate for $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ choosing as `log_prior` function a (log-)Standard Normal distribution and as `log_likelihood` function the (log-)Skellam. Perform optimization using Gradient Descent;\n",
    "\n",
    "def log_likelihood(teams_A, teams_B, goal_diff, theta):\n",
    "    # TODO\n",
    "    return ...\n",
    "\n",
    "def log_prior(theta):\n",
    "    # TODO\n",
    "    return ...\n",
    "\n",
    "def loss_function(theta):\n",
    "    # TODO \n",
    "    return ...\n",
    "\n",
    "def gradient_descent_optimization(loss_function, lr: float, n_iter: int, initial_guess: torch.Tensor):\n",
    "    # TODO\n",
    "    return ...\n",
    "\n",
    "def compute_hessian(f, w):\n",
    "    # TODO\n",
    "    return ...\n",
    "\n",
    "# here we want to obtain \n",
    "theta_MAP = ...\n",
    "posterior_cov = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the marginal distribution of some thetas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Sample from the full posterior\n",
    "mvn = MultivariateNormal(loc=theta_MAP, covariance_matrix=torch.from_numpy(posterior_cov).float())\n",
    "posterior_samples = mvn.sample((1000,)) \n",
    "\n",
    "# Indices of thetas you want to visualize\n",
    "selected_indices = [0, 1, 2, 20]  # Change these to the indices you're interested in\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.hist(posterior_samples[:, idx].numpy(), bins=40, density=True, alpha=0.7)\n",
    "    plt.title(f\"Posterior of $\\\\theta_{{{idx}}}$\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement your `Skellam` distribution, inheriting from `torch.distributions.Distribution`;\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "class Skellam(torch.distributions.Distribution):\n",
    "    arg_constraints = ...\n",
    "\n",
    "    def __init__(self, lambdaA, lambdaB, validate_args = None):\n",
    "        self.lambdaA = lambdaA\n",
    "        self.lambdaB = lambdaB\n",
    "        batch_shape = torch.broadcast_shapes(lambdaA.shape, lambdaB.shape)\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        # Approximate sampling: sample two Poisson and subtract\n",
    "        # TODO\n",
    "        return ...\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        # Exact log probability\n",
    "        return ...\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 3. Write the Pyro model corresponding to the problem depicted above assuming (again) the `theta` values being distributed initially as a Standard Normal;\n",
    "\n",
    "def model(goal_diff, teams_A, teams_B, max_player_id):\n",
    "    # TODO\n",
    "    ...\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 4. Perform inference on $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ values running a MCMC simulation using the `NUTS` kernel;\n",
    "\n",
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "# TODO\n",
    "\n",
    "theta_MCMC = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Compare the `theta_*` values obtained by these two options using the `performances_evaluation` function given in this notebook.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_goal_diff_skellam(teamA_ids, teamB_ids, theta, n_sim=10_000, posterior_cov=None):\n",
    "    \"\"\"\n",
    "    Predicts the goal difference (Skellam distribution) between two teams using theta.\n",
    "\n",
    "    Args:\n",
    "        teamA_ids (list): Indices of the players in team A.\n",
    "        teamB_ids (list): Indices of the players in team B.\n",
    "        theta (torch.Tensor): Tensor containing the skill levels of the players.\n",
    "        n_sim (int): Number of simulations to run.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean predicted goal difference (A - B).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the total strength for each team (sum of player skill levels)\n",
    "    sA = theta[teamA_ids].sum()\n",
    "    sB = theta[teamB_ids].sum()\n",
    "\n",
    "    # Lambda parameters for Poisson distribution (expected goals)\n",
    "    lam_A = torch.exp(sA).item()  # Team A's expected goal rate\n",
    "    lam_B = torch.exp(sB).item()  # Team B's expected goal rate\n",
    "\n",
    "    # Simulate goals for each team using Poisson distribution\n",
    "    goals_A = np.random.poisson(lam_A, size=n_sim)\n",
    "    goals_B = np.random.poisson(lam_B, size=n_sim)\n",
    "\n",
    "    # Calculate the difference in goals (A - B)\n",
    "    diff = goals_A - goals_B\n",
    "\n",
    "    # Return the mean predicted difference\n",
    "    return diff.mean()\n",
    "\n",
    "\n",
    "def predict_goal_diff_laplace(teamA_ids, teamB_ids, theta_map, cov_matrix, n_sim=1000):\n",
    "    # Combine all relevant indices\n",
    "    all_ids = torch.tensor(sorted(set(teamA_ids) | set(teamB_ids)))\n",
    "\n",
    "    # Extract subvector of means (mu_A and mu_B)\n",
    "    theta_sub = theta_map[all_ids].float()\n",
    "\n",
    "    # Extract submatrix of covariances\n",
    "    cov_sub = cov_matrix[np.ix_(all_ids, all_ids)]  # numpy version\n",
    "    cov_sub = torch.from_numpy(cov_sub).float()\n",
    "\n",
    "    # Create multivariate normal from posterior\n",
    "    mvn = MultivariateNormal(loc=theta_sub, covariance_matrix=cov_sub)\n",
    "\n",
    "    # Sample from posterior\n",
    "    theta_samples = mvn.sample((n_sim,))  # shape: (n_sim, len(all_ids))\n",
    "\n",
    "    # Compute strength for each team in each sample\n",
    "    teamA_len = len(teamA_ids)\n",
    "    sA = theta_samples[:, :teamA_len].sum(dim=1)\n",
    "    sB = theta_samples[:, teamA_len:].sum(dim=1)\n",
    "\n",
    "    # Expected goals via Poisson lambdas\n",
    "    lam_A = torch.exp(sA)\n",
    "    lam_B = torch.exp(sB)\n",
    "\n",
    "    # Sample goals\n",
    "    goals_A = torch.poisson(lam_A)\n",
    "    goals_B = torch.poisson(lam_B)\n",
    "\n",
    "    # Goal differences\n",
    "    diff = goals_A - goals_B\n",
    "    return diff.mean()\n",
    "\n",
    "\n",
    "def evaluate_performance(theta_MAP, theta_MCMC, posterior_cov, teams_A, teams_B, goal_diff, n_sim=10_000):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of different sets of theta values by comparing the Mean Bias Error (MBE)\n",
    "    and Mean Absolute Error (MAE) between predicted and actual goal differences.\n",
    "\n",
    "    Args:\n",
    "        theta_values (list of torch.Tensor): List of different theta values to evaluate.\n",
    "        teams_A (list of lists): List of player IDs for team A in each match.\n",
    "        teams_B (list of lists): List of player IDs for team B in each match.\n",
    "        goal_diff (list): List of actual goal differences (A - B).\n",
    "        n_sim (int): Number of simulations to run for each set of theta values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with MBE and MAE for each set of theta values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each set of theta values\n",
    " \n",
    "    predicted_diffs_MAP = []  # List to store predicted goal differences\n",
    "    predicted_diffs_MCMC = []\n",
    "    actual_diffs = goal_diff  # Actual goal differences\n",
    "\n",
    "    # Simulate the match results for each game\n",
    "    for i in range(len(goal_diff)):\n",
    "        teamA_ids = teams_A[i]\n",
    "        teamB_ids = teams_B[i]\n",
    "\n",
    "        # Predict the goal difference \n",
    "        # Use the Laplace approximation if posterior_cov is provided\n",
    "        \n",
    "        predicted_MAP = predict_goal_diff_laplace(teamA_ids, teamB_ids, theta_MAP, posterior_cov, n_sim)\n",
    "        predicted_MCMC = predict_goal_diff_skellam(teamA_ids, teamB_ids, theta_MCMC, n_sim)\n",
    "        \n",
    "        predicted_diffs_MAP.append(predicted_MAP)\n",
    "        predicted_diffs_MCMC.append(predicted_MCMC)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier calculations\n",
    "    predicted_diffs_MAP = np.array(predicted_diffs_MAP)\n",
    "    predicted_diffs_MCMC = np.array(predicted_diffs_MCMC)\n",
    "    actual_diffs = np.array(actual_diffs)\n",
    "\n",
    "    # Compute MAE and MBE\n",
    "    mae_MAP = np.mean(np.abs(predicted_diffs_MAP - actual_diffs))  # Mean Absolute Error\n",
    "    mbe_MAP = (predicted_diffs_MAP - actual_diffs).mean()  # Mean Bias Error\n",
    "    mae_MCMC = np.mean(np.abs(predicted_diffs_MCMC - actual_diffs))  # Mean Absolute Error\n",
    "    mbe_MCMC = (predicted_diffs_MCMC - actual_diffs).mean()  # Mean Bias Error\n",
    "\n",
    "    # Store results for the current theta set\n",
    "    results = {'MAE_MAP': mae_MAP, 'MBE_MAP': mbe_MAP,\n",
    "              'MAE_MCMC': mae_MCMC, 'MBE_MCMC': mbe_MCMC}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate the performance of different theta values on the same dataset used to fix those values\n",
    "performance_results = evaluate_performance(theta_MCMC, theta_MAP, posterior_cov, teams_A, teams_B, goal_diff)\n",
    "\n",
    "# Print the results\n",
    "print(\"Performance Results:\")\n",
    "print(f\"MAP - MAE: {performance_results['MAE_MAP']}, MBE: {performance_results['MBE_MAP']}\")\n",
    "print(f\"MCMC - MAE: {performance_results['MAE_MCMC']}, MBE: {performance_results['MBE_MCMC']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
