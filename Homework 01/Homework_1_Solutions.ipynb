{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 Solutions\n",
    "\n",
    "Probabilistic Machine Learning -- Spring 2025, UniTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: KL Divergence Between Two Gaussian Distributions\n",
    "\n",
    "Given two multivariate Gaussian distributions:\n",
    "\n",
    "$$p(x) = \\mathcal{N}(x \\mid \\mu, \\Sigma)$$\n",
    "\n",
    "$$q(x) = \\mathcal{N}(x \\mid m, L)$$\n",
    "\n",
    "where:\n",
    "- $\\mu$ and $\\Sigma$ are the mean vector and covariance matrix of $p(x)$\n",
    "- $m$ and $L$ are the mean vector and covariance matrix of $q(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Derive the closed-form expression for $D_{KL}(p \\parallel q)$ starting from the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the definition of KL divergence:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "\n",
    "For multivariate Gaussian distributions, we have:\n",
    "\n",
    "$$p(x) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$\n",
    "\n",
    "$$q(x) = \\frac{1}{(2\\pi)^{d/2}|L|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-m)^TL^{-1}(x-m)\\right)$$\n",
    "\n",
    "where $d$ is the dimension of the random variable $x$.\n",
    "\n",
    "Substituting these into the KL divergence formula:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "\n",
    "$$= \\int p(x) \\log \\left[ \\frac{\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)}{\\frac{1}{(2\\pi)^{d/2}|L|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-m)^TL^{-1}(x-m)\\right)} \\right] dx$$\n",
    "\n",
    "$$= \\int p(x) \\log \\left[ \\frac{|L|^{1/2}}{|\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu) + \\frac{1}{2}(x-m)^TL^{-1}(x-m)\\right) \\right] dx$$\n",
    "\n",
    "$$= \\int p(x) \\left[ \\frac{1}{2}\\log\\frac{|L|}{|\\Sigma|} -\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu) + \\frac{1}{2}(x-m)^TL^{-1}(x-m) \\right] dx$$\n",
    "\n",
    "Let's compute each term separately:\n",
    "\n",
    "1. $\\frac{1}{2}\\log\\frac{|L|}{|\\Sigma|} \\int p(x) dx = \\frac{1}{2}\\log\\frac{|L|}{|\\Sigma|}$ (since $\\int p(x) dx = 1$)\n",
    "\n",
    "2. $-\\frac{1}{2} \\int p(x) (x-\\mu)^T\\Sigma^{-1}(x-\\mu) dx = -\\frac{1}{2} \\mathbb{E}_{p(x)}[(x-\\mu)^T\\Sigma^{-1}(x-\\mu)] = -\\frac{1}{2} \\text{tr}(\\Sigma^{-1}\\Sigma) = -\\frac{d}{2}$\n",
    "\n",
    "3. For the third term, we need to expand $(x-m)^TL^{-1}(x-m)$:\n",
    "   \n",
    "   $(x-m)^TL^{-1}(x-m) = (x-\\mu+\\mu-m)^TL^{-1}(x-\\mu+\\mu-m)$\n",
    "   \n",
    "   $= (x-\\mu)^TL^{-1}(x-\\mu) + 2(x-\\mu)^TL^{-1}(\\mu-m) + (\\mu-m)^TL^{-1}(\\mu-m)$\n",
    "\n",
    "   Taking the expectation with respect to $p(x)$:\n",
    "   \n",
    "   $\\mathbb{E}_{p(x)}[(x-\\mu)^TL^{-1}(x-\\mu)] = \\text{tr}(L^{-1}\\Sigma)$\n",
    "   \n",
    "   $\\mathbb{E}_{p(x)}[2(x-\\mu)^TL^{-1}(\\mu-m)] = 0$ (since $\\mathbb{E}_{p(x)}[x-\\mu] = 0$)\n",
    "   \n",
    "   $\\mathbb{E}_{p(x)}[(\\mu-m)^TL^{-1}(\\mu-m)] = (\\mu-m)^TL^{-1}(\\mu-m)$\n",
    "\n",
    "Combining all terms:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\frac{1}{2}\\log\\frac{|L|}{|\\Sigma|} - \\frac{d}{2} + \\frac{1}{2}\\text{tr}(L^{-1}\\Sigma) + \\frac{1}{2}(\\mu-m)^TL^{-1}(\\mu-m)$$\n",
    "\n",
    "$$= \\frac{1}{2}\\left[ \\log\\frac{|L|}{|\\Sigma|} - d + \\text{tr}(L^{-1}\\Sigma) + (\\mu-m)^TL^{-1}(\\mu-m) \\right]$$\n",
    "\n",
    "This is the closed-form expression for the KL divergence between two multivariate Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement a Python function that computes the closed-form expression of the KL divergence for two-dimensional Gaussian distributions using only numpy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between two multivariate Gaussian distributions p and q.\n",
    "    \n",
    "    Parameters:\n",
    "    mu_p (numpy.ndarray): Mean vector of distribution p\n",
    "    sigma_p (numpy.ndarray): Covariance matrix of distribution p\n",
    "    mu_q (numpy.ndarray): Mean vector of distribution q\n",
    "    sigma_q (numpy.ndarray): Covariance matrix of distribution q\n",
    "    \n",
    "    Returns:\n",
    "    float: KL divergence D_KL(p||q)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    mu_p = np.asarray(mu_p)\n",
    "    mu_q = np.asarray(mu_q)\n",
    "    sigma_p = np.asarray(sigma_p)\n",
    "    sigma_q = np.asarray(sigma_q)\n",
    "    \n",
    "    # Get dimension\n",
    "    d = len(mu_p)\n",
    "    \n",
    "    # Calculate log determinant ratio\n",
    "    log_det_ratio = np.log(np.linalg.det(sigma_q) / np.linalg.det(sigma_p))\n",
    "    \n",
    "    # Calculate trace term\n",
    "    sigma_q_inv = np.linalg.inv(sigma_q)\n",
    "    trace_term = np.trace(np.matmul(sigma_q_inv, sigma_p))\n",
    "    \n",
    "    # Calculate mean difference term\n",
    "    mean_diff = mu_p - mu_q\n",
    "    mean_term = np.matmul(np.matmul(mean_diff.T, sigma_q_inv), mean_diff)\n",
    "    \n",
    "    # Combine all terms\n",
    "    kl = 0.5 * (log_det_ratio - d + trace_term + mean_term)\n",
    "    \n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test the function on the following concrete example where both p(x) and q(x) are two-dimensional Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "mu_p = np.array([10, 12])\n",
    "sigma_p = np.array([[3, 0.5], [0.5, 2]])\n",
    "mu_q = np.array([14, 10])\n",
    "sigma_q = np.array([[2, 0.3], [0.3, 1]])\n",
    "\n",
    "data_points = 1000\n",
    "\n",
    "p_samples = np.random.multivariate_normal(mu_p, sigma_p, data_points)\n",
    "q_samples = np.random.multivariate_normal(mu_q, sigma_q, data_points)\n",
    "\n",
    "# p, q samples visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(p_samples[:, 0], p_samples[:, 1], alpha=0.5, label='p(x)', color='blue')\n",
    "plt.scatter(q_samples[:, 0], q_samples[:, 1], alpha=0.5, label='q(x)', color='red')\n",
    "plt.title('Samples drawn from two multivariate Gaussians')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate KL divergence using our function\n",
    "kl_div = kl_divergence_gaussian(mu_p, sigma_p, mu_q, sigma_q)\n",
    "print(f\"KL divergence D_KL(p||q) = {kl_div:.6f}\")\n",
    "\n",
    "# Also calculate the reverse KL divergence to show asymmetry\n",
    "kl_div_reverse = kl_divergence_gaussian(mu_q, sigma_q, mu_p, sigma_p)\n",
    "print(f\"KL divergence D_KL(q||p) = {kl_div_reverse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement another Python function which calculates an approximation of $D_{KL}(p \\parallel q)$ from samples of p and q. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_from_samples(p_samples, q_samples, bins=20):\n",
    "    \"\"\"\n",
    "    Approximate KL divergence from samples using histogram binning.\n",
    "    \n",
    "    Parameters:\n",
    "    p_samples (numpy.ndarray): Samples from distribution p\n",
    "    q_samples (numpy.ndarray): Samples from distribution q\n",
    "    bins (int): Number of bins for histogram\n",
    "    \n",
    "    Returns:\n",
    "    float: Approximated KL divergence D_KL(p||q)\n",
    "    \"\"\"\n",
    "    # Determine the range for histograms\n",
    "    min_x = min(np.min(p_samples[:, 0]), np.min(q_samples[:, 0]))\n",
    "    max_x = max(np.max(p_samples[:, 0]), np.max(q_samples[:, 0]))\n",
    "    min_y = min(np.min(p_samples[:, 1]), np.min(q_samples[:, 1]))\n",
    "    max_y = max(np.max(p_samples[:, 1]), np.max(q_samples[:, 1]))\n",
    "    \n",
    "    # Create 2D histograms\n",
    "    range_xy = [[min_x, max_x], [min_y, max_y]]\n",
    "    p_hist, x_edges, y_edges = np.histogram2d(p_samples[:, 0], p_samples[:, 1], bins=bins, range=range_xy, density=True)\n",
    "    q_hist, _, _ = np.histogram2d(q_samples[:, 0], q_samples[:, 1], bins=bins, range=range_xy, density=True)\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero or log(0)\n",
    "    epsilon = 1e-10\n",
    "    p_hist = p_hist + epsilon\n",
    "    q_hist = q_hist + epsilon\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    kl_div = np.sum(p_hist * np.log(p_hist / q_hist)) * ((max_x - min_x) / bins) * ((max_y - min_y) / bins)\n",
    "    \n",
    "    return kl_div\n",
    "\n",
    "# Calculate KL divergence from samples\n",
    "kl_div_samples = kl_divergence_from_samples(p_samples, q_samples, bins=30)\n",
    "print(f\"KL divergence from samples D_KL(p||q) â‰ˆ {kl_div_samples:.6f}\")\n",
    "\n",
    "# Compare with analytical result\n",
    "print(f\"Analytical KL divergence D_KL(p||q) = {kl_div:.6f}\")\n",
    "print(f\"Difference: {abs(kl_div - kl_div_samples):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Entropy of nonsingular linear transformations\n",
    "\n",
    "Consider a vector $x$ of continuous variables with distribution $p(x)$ and corresponding entropy $H[x]$. Suppose that we make a nonsingular linear transformation of $x$ to obtain a new variable $y = Ax$. Show that the corresponding entropy is given by\n",
    "\n",
    "$$H[y] = H[x] + \\ln |A|$$\n",
    "\n",
    "where $|A|$ denotes the determinant of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "\n",
    "Let's start with the definition of entropy for a continuous random variable $x$ with probability density function $p(x)$:\n",
    "\n",
    "$$H[x] = -\\int p(x) \\ln p(x) dx$$\n",
    "\n",
    "Now, we want to find the entropy of $y = Ax$, where $A$ is a nonsingular (invertible) matrix.\n",
    "\n",
    "First, we need to find the probability density function of $y$, which we'll denote as $p_y(y)$. We can use the change of variables formula from probability theory.\n",
    "\n",
    "Since $y = Ax$, we have $x = A^{-1}y$. The Jacobian of this transformation is $J = |\\frac{\\partial x}{\\partial y}| = |A^{-1}| = \\frac{1}{|A|}$, where $|A|$ is the determinant of $A$.\n",
    "\n",
    "By the change of variables formula, we have:\n",
    "\n",
    "$$p_y(y) = p_x(A^{-1}y) \\cdot |A^{-1}| = p_x(A^{-1}y) \\cdot \\frac{1}{|A|}$$\n",
    "\n",
    "Now, let's compute the entropy of $y$:\n",
    "\n",
    "$$H[y] = -\\int p_y(y) \\ln p_y(y) dy$$\n",
    "\n",
    "Substituting the expression for $p_y(y)$:\n",
    "\n",
    "$$H[y] = -\\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\ln \\left( p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\right) dy$$\n",
    "\n",
    "Using the properties of logarithms:\n",
    "\n",
    "$$H[y] = -\\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\left[ \\ln p_x(A^{-1}y) + \\ln \\frac{1}{|A|} \\right] dy$$\n",
    "\n",
    "$$H[y] = -\\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\ln p_x(A^{-1}y) dy - \\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\ln \\frac{1}{|A|} dy$$\n",
    "\n",
    "Let's make a change of variables in the first integral: $u = A^{-1}y$, which implies $y = Au$ and $dy = |A| du$. Substituting:\n",
    "\n",
    "$$H[y] = -\\int p_x(u) \\cdot \\frac{1}{|A|} \\ln p_x(u) |A| du - \\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} \\ln \\frac{1}{|A|} dy$$\n",
    "\n",
    "$$H[y] = -\\int p_x(u) \\ln p_x(u) du - \\ln \\frac{1}{|A|} \\int p_x(A^{-1}y) \\cdot \\frac{1}{|A|} dy$$\n",
    "\n",
    "The first term is just $H[x]$. For the second term, we can again make the change of variables $u = A^{-1}y$, which gives:\n",
    "\n",
    "$$H[y] = H[x] - \\ln \\frac{1}{|A|} \\int p_x(u) du$$\n",
    "\n",
    "Since $p_x(u)$ is a probability density function, $\\int p_x(u) du = 1$. Therefore:\n",
    "\n",
    "$$H[y] = H[x] - \\ln \\frac{1}{|A|} = H[x] + \\ln |A|$$\n",
    "\n",
    "This proves the desired result: $H[y] = H[x] + \\ln |A|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: A good reason to go to university\n",
    "\n",
    "You enrolled to a small tennis tournament organized by your university, that has only other three participants: let's call them $A$, $B$ and $C$. Your first match will be against $A$, and it's scheduled after the match between $A$ and $B$ and the match between $B$ and $C$.\n",
    "\n",
    "Assuming the result of a match $M \\in \\{0, 1\\}$ between two players $X$ and $Y$ ($M = 1$ means $X$ won, $M = 0$ means $Y$ won) is described by the following model:\n",
    "\n",
    "$$M \\sim Bern(p)$$\n",
    "\n",
    "where $p = f(2(S_x - S_y))$ with $f(k) = \\frac{1}{1+e^{-k}}$ and\n",
    "\n",
    "$$S_i \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "is the \"latent\" skill of player $i$ (always the same for every match that player $i$ plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Show a bayesian network describing the relationship between all the involved random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian network for this tennis tournament scenario involves the following random variables:\n",
    "- $S_A$, $S_B$, $S_C$, $S_{You}$: The latent skills of players A, B, C, and You\n",
    "- $M_{AB}$: Result of match between A and B\n",
    "- $M_{BC}$: Result of match between B and C\n",
    "- $M_{AY}$: Result of match between A and You\n",
    "\n",
    "The Bayesian network structure is as follows:\n",
    "\n",
    "```\n",
    "S_A    S_B    S_C    S_You\n",
    " |\\     /|     /       |\n",
    " | \\   / |    /        |\n",
    " |  \\ /  |   /         |\n",
    " |   X   |  /          |\n",
    " |  / \\  | /           |\n",
    " | /   \\ |/            |\n",
    "M_AB    M_BC          /\n",
    "         \\           /\n",
    "          \\         /\n",
    "           \\       /\n",
    "            \\     /\n",
    "             \\   /\n",
    "              \\ /\n",
    "             M_AY\n",
    "```\n",
    "\n",
    "In this network:\n",
    "- Each skill variable $S_i$ follows a standard normal distribution $\\mathcal{N}(0, 1)$\n",
    "- Match results depend on the skills of the two players involved\n",
    "- $M_{AB}$ depends on $S_A$ and $S_B$\n",
    "- $M_{BC}$ depends on $S_B$ and $S_C$\n",
    "- $M_{AY}$ depends on $S_A$ and $S_{You}$\n",
    "\n",
    "The conditional probability distributions are:\n",
    "- $P(M_{AB} = 1 | S_A, S_B) = f(2(S_A - S_B))$\n",
    "- $P(M_{BC} = 1 | S_B, S_C) = f(2(S_B - S_C))$\n",
    "- $P(M_{AY} = 1 | S_A, S_{You}) = f(2(S_A - S_{You}))$\n",
    "\n",
    "where $f(k) = \\frac{1}{1+e^{-k}}$ is the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make a model in pyro describing the stochastic process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_se
(Content truncated due to size limit. Use line ranges to read in chunks)